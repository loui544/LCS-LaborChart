"""
Skill tagger global utilities,

Functions for:
    Wordpiece tokenizer merging:
        BERT's variants uses WordPiece tokenizer. Huggingface token-classification
        pipeline returns WordPiece pieces. That pieces need to be merged to improve
        Gradio UI demo.

"""

import re
import numpy as np
from typing import List, Dict


def clean_label(label: str):
    """
    Removes O-, B-, I-, L-, U- prefixes from NER tags

    Parameters
    ----------
    label : str
        NER tag with prefix

    Returns
    -------
    str
        NER tag without prefix
    """

    if re.match("^[BOILU]-",label):
        return "-".join(label.split('-')[1:])
    else:
        return label


def check_actual_and_next(predicted_labels: List[Dict], i: int):

    """
    Checks wheter if the i and i+1 tokens have the ##prefix
    defined by WordPiece tokenizer 
    
    Parameters
    ----------
    predicted_labels : List[Dict]
        List of dict with predicted labels for each token
        [
            {'entity': 'optional', 'score': float (optional), 'index': int (optional), 
            'word': 'requ', 'start': int (optional), 'end': int (optional)},
            {'entity': 'optional', 'score': float (optional), 'index': int (optional), 
            'word': '##ired', 'start': int (optional), 'end': int (optional)},
        ]
    i: int
        Actual token index

    Returns
    -------
    re.Match
        is_splitted, If actual token is a splitted one else None
    re.Match
        is_next_splitted, If next token is a splitted one else None
    """

    n = len(predicted_labels)-1
    
    ## Get actual and next word
    iword = predicted_labels[i].get('word')
    if i<n:
        i1word = predicted_labels[i+1].get('word')
    else:
        i1word = ''
    
    is_splitted = re.match('^##.*',iword)
    is_next_splitted = re.match('^##.*',i1word)

    return is_splitted, is_next_splitted


def group_wordpiece_tokens(predicted_labels: List[Dict]) -> List[List[Dict]]:

    """
    Groups wordpiece tokens. The Predicted labels come in a list of predictions made by 
    the Hugginface token-classification pipeline. The predicted_labels must be the result
    of a Pipeline that doesn't ignore any label (must include 'O' label) 
    and all the words available within the original string.

    Parameters
    ----------
    predicted_labels: List[Dict]
        list of tokens and predictions made by Hugginface token-classification pipeline

    Returns
    -------
    List[List[Dict]]
        List of grouped tokens and predictions
    """
    

    grouped_predicted_labels = []
    i = 0
    n = len(predicted_labels)-1
    while i<=n:

        # Check if actual and next word are splitted
        is_splitted, is_next_splitted = check_actual_and_next(predicted_labels,i)

        ## If actual word and next word are not splitted, actual word is not
        if not is_splitted and not is_next_splitted:

            # Append lonely word as a group
            token_parts = [predicted_labels[i]]

            # Add to counter
            i += 1
    
        ## If actual word is not splitted but next one is, actual word could be splitted
        elif not is_splitted and is_next_splitted:

            # Append first word to the group
            token_parts = [predicted_labels[i]]

            # Instantiate Flag
            word_group_end = False

            while word_group_end == False:

                # Add to counter
                i += 1
                
                # Append next word to the group
                token_parts.append(predicted_labels[i])

                # Check if actual and next word are splitted
                is_splitted, is_next_splitted = check_actual_and_next(predicted_labels,i)

                # We are on a splitted word, if both are splitted continue adding to the group
                # If actual word is ##splitted but next one isnt, stop iteration
                if is_splitted and not is_next_splitted:
                    # Add to counter
                    i += 1

                    # # Append next word to the group
                    # token_parts.append(predicted_labels[i])
                    
                    word_group_end = True

        ## Append grouped token parts
        grouped_predicted_labels.append(token_parts)
    
    return grouped_predicted_labels     


def get_grouped_token_entity(grouped_token: List[Dict]):
    """Selects overall predicted token entity based on 
        wordpiece pieces and predicted entities.

    Parameters
    ----------
    grouped_token : List[Dict]
        List of wordpiece splitted tokens generated by Hugginface's 
        token-clasification pipeline

    Returns
    -------
    str:
        Group entity
    float:
        Entity score
    """

    argmax_entity = 'O'
    argmax_score = 0
    o_score = 0
    for token_piece in grouped_token:
        piece_entity = clean_label(token_piece['entity'])
        piece_score = token_piece['score']
        if piece_entity != 'O' and piece_score>argmax_score:
            argmax_entity = piece_entity
            argmax_score = piece_score
        else:
            o_score = piece_score

    if argmax_entity=='O':
        argmax_score = o_score

    return argmax_entity, argmax_score


def merge_grouped_tokens(grouped_predicted_labels: List[List[Dict]],sep: str ='') -> List[Dict]:

    """Merges wordpiece splitted tokens and Hugginface's token-clasification pipeline
        predicted entities and scores

    Parameters
    ----------
    grouped_predicted_labels: List[List[Dict]]
        list of lists with grouped wordpiece tokens
    sep: str, defaults to "" empty string
        String separator to merge tokens 

    Returns
    -------
    List[Dict]
        List of merged worpiece pieces
    """

    merged_tokens = []
    for i, grouped_token in enumerate(grouped_predicted_labels):

        argmax_entity, argmax_score = get_grouped_token_entity(grouped_token=grouped_token)
        word = f"{sep}".join([a['word'].replace('##','') for a in grouped_token])
        start_index = grouped_token[0]['start']
        end_index = grouped_token[-1]['end']

        merged_tokens.append(
            {
                'entity': argmax_entity,
                'score': argmax_score,
                'index': i,
                'word': word,
                'start': start_index,
                'end': end_index
            }
        )
    
    return merged_tokens


def group_adjacent_labels(merged_tokens: List[Dict]) -> List[List[Dict]]:
    """Groups adjacent tokens with same label. Tokens labels must be consecutive
    to be considered as a same entity.

    Parameters
    ----------
    merged_tokens : List[Dict]
        List of pre-merged tokens after BERT model prediction, coming
        from HUgginface's Token Clasification pipeline.

    Returns
    -------
    List[List[Dict]]
        Nested lists wit grouped tokens
    """

    grouped_merged_tokens = []
    i = 0
    n = len(merged_tokens)-1
    while i<=n:
        ## If there is no such entity, group token with itself
        if merged_tokens[i]['entity'] == 'O':
            grouped_merged_tokens.append(
                [merged_tokens[i]]
            )
            ## Increase counter, to allow next token to be checked and grouped.
            i += 1
        ## If not, check if adjacent tokens have same label and group them
        else:
            merged_tokens_group = []
            ## Add current token to the group
            merged_tokens_group.append(
                merged_tokens[i]
            )
            adjacent_labels_end = False
            while adjacent_labels_end == False:
                ## Check if next token shares same label
                if i<n and merged_tokens[i]['entity'] == merged_tokens[i+1]['entity']:
                    ## Increase counter,
                    i += 1
                    ## Add next similar token to the group
                    merged_tokens_group.append(
                        merged_tokens[i]
                    )
                else:
                    ## Increase counter, to allow next token to be checked and grouped.
                    i += 1
                    ## Flag end of adjacent labels
                    adjacent_labels_end = True
            ## Append grouped tokens to global list
            grouped_merged_tokens.append(merged_tokens_group)

    return grouped_merged_tokens


def gen_random_examples(valid: List[str] = None,n: int = 5) -> List[List[str]]:
    """
    Generates job offers random sample coming from valid test. If file not available
    returns an empty list.

    Parameters
    ----------
    valid : List[str]
        List of valid examples
    n: int
        Sample size

    Returns
    -------
    List[List[str]]
        List of lists, each inner list contains 1 Random sample of size 1 from texts
    """

    ## List texts and extract a random sample
    if valid:
        valid_texts = [a['content'] for a in valid]
        valid_texts_sample = np.random.choice(valid_texts,size=10,replace=False)
        valid_texts_sample = [[t] for t in valid_texts_sample]
    else:
        valid_texts_sample = []
    
    return valid_texts_sample



def ner_cleaning(predicted_labels: List[Dict]):
    """
    Job skills named entity recognition post-processing. The entities without any assigned entity

    Parameters
    ----------
    predicted_labels: List[Dict]
        list of tokens and predictions made by Hugginface token-classification pipeline

    Returns
    -------
    List[Dict]
        List of grouped tokens, labeled by the model
    """
    
    # Group Wordpiece Tokens
    grouped_predicted_labels = group_wordpiece_tokens(predicted_labels)
    # Merge Grouped wordpiece Tokens
    merged_tokens = merge_grouped_tokens(grouped_predicted_labels=grouped_predicted_labels)
    # Group adjacent tokens based on predicted labels
    grouped_merged_tokens = group_adjacent_labels(merged_tokens)
    # Merge adjacent tokens and labels
    grouped_merged_tokens = merge_grouped_tokens(grouped_predicted_labels=grouped_merged_tokens, sep=" ")
    # Remove 'O' entities
    grouped_merged_tokens = [
        token for token in grouped_merged_tokens
        if token['entity']!='O'
    ]
    return grouped_merged_tokens

